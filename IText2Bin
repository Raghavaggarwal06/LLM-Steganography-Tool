"""
IText2Bin: Text compression using llama-zip with Modal GPU Container

This module provides a function to compress text using llama-zip compression tool
hosted on Modal GPU containers for fast inference.
"""

import modal
import subprocess
import base64
import tempfile
import os
from pathlib import Path

# --- CONFIGURATION ---
# Q4_K_M model - using a common model that supports the required context
# You may need to adjust this path or download the model
MODEL_NAME = "llama-3.1-8b-instruct-v0.1.Q4_K_M.gguf"
MODEL_URL = "https://huggingface.co/bartowski/Llama-3.1-8B-Instruct-GGUF/resolve/main/Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# Modal image with llama-zip dependencies
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git", "build-essential", "cmake")
    .pip_install(
        "llama-cpp-python[cuda]",
        "requests",
    )
    .run_commands(
        "git clone https://github.com/AlexBuz/llama-zip.git /root/llama-zip",
        "cd /root/llama-zip && pip install -r requirements.txt",
    )
)

app = modal.App("llama-zip-compression", image=image)

# Volume for model storage (persists across runs)
model_volume = modal.Volume.from_name("llama-zip-models", create_if_missing=True)
MODEL_PATH = "/models/llama-3.1-8b-instruct-v0.1.Q4_K_M.gguf"


@app.cls(
    gpu="A10G",  # GPU for fast inference
    timeout=600,
    volumes={"/models": model_volume},
)
class LlamaZipCompressor:
    
    @modal.enter()
    def setup(self):
        """Download and setup the Q4_K_M model if not already present."""
        import requests
        
        if not os.path.exists(MODEL_PATH):
            print(f"Downloading model {MODEL_NAME}...")
            os.makedirs("/models", exist_ok=True)
            
            # Download model
            response = requests.get(MODEL_URL, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(MODEL_PATH, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            if downloaded % (1024 * 1024) == 0:  # Print every MB
                                print(f"Downloaded: {downloaded / (1024*1024):.1f} MB ({percent:.1f}%)")
            
            print(f"Model downloaded successfully to {MODEL_PATH}")
            model_volume.commit()
        else:
            print(f"Model already exists at {MODEL_PATH}")
    
    @modal.method()
    def IText2Bin(self, input_text: str) -> str:
        """
        Compress input text using llama-zip and return base64-encoded result with 8-bit header.
        
        Args:
            input_text: The text to compress
            
        Returns:
            Base64-encoded string with 8-bit header containing the length of encoded bits
        """
        # Create temporary files for input and output
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as input_file:
            input_file.write(input_text)
            input_path = input_file.name
        
        output_path = input_path + ".llama_zip"
        
        try:
            # Run llama-zip compression
            # Using --n-ctx 8192 and -w 25% (window overlap)
            cmd = [
                "python", "/root/llama-zip/llama_zip.py",
                "-m", MODEL_PATH,
                "-i", input_path,
                "-o", output_path,
                "--n-ctx", "8192",
                "-w", "25%",  # 25% window overlap
            ]
            
            print(f"Running compression command: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True,
                cwd="/root/llama-zip"
            )
            
            print(f"Compression stdout: {result.stdout}")
            if result.stderr:
                print(f"Compression stderr: {result.stderr}")
            
            # Read the compressed binary data
            with open(output_path, 'rb') as f:
                compressed_data = f.read()
            
            # Calculate the length of compressed data in bits
            bit_length = len(compressed_data) * 8
            
            # Create 8-bit header with the length of encoded bits
            # Note: 8 bits can only represent 0-255, which limits to ~31 bytes of compressed data
            # If the compressed data is larger, we use modulo 256 (wraps around)
            # For production use, consider using 16-bit or 32-bit header
            if bit_length > 255:
                bit_length_8bit = bit_length % 256
                print(f"Warning: Bit length {bit_length} exceeds 8-bit limit (255), using {bit_length_8bit} (wrapped)")
            else:
                bit_length_8bit = bit_length
            
            # Create 8-bit header (single byte representing bit length)
            header_byte = bytes([bit_length_8bit])
            
            # Combine header + compressed binary data
            # The bitstring is: [8-bit header][compressed binary data]
            bitstring = header_byte + compressed_data
            
            # Convert the entire bitstring (header + compressed data) to base64
            # This gives us base64 encoded bytes as requested
            base64_encoded = base64.b64encode(bitstring).decode('utf-8')
            
            return base64_encoded
            
        except subprocess.CalledProcessError as e:
            error_msg = f"Compression failed: {e.stderr}"
            print(error_msg)
            raise RuntimeError(error_msg)
        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            print(error_msg)
            raise RuntimeError(error_msg)
        finally:
            # Cleanup temporary files
            try:
                if os.path.exists(input_path):
                    os.unlink(input_path)
                if os.path.exists(output_path):
                    os.unlink(output_path)
            except:
                pass


# Convenience function for local testing
def IText2Bin(input_text: str) -> str:
    """
    Convenience wrapper function that can be called directly.
    Requires Modal app to be running.
    """
    with app.run():
        compressor = LlamaZipCompressor()
        return compressor.IText2Bin.remote(input_text)


if __name__ == "__main__":
    # Example usage
    test_text = "This is a test message to compress using llama-zip compression tool."
    
    print("Testing IText2Bin compression...")
    print(f"Input text: {test_text}")
    print(f"Input length: {len(test_text)} characters")
    
    with app.run():
        compressor = LlamaZipCompressor()
        result = compressor.IText2Bin.remote(test_text)
        
        print(f"\nCompressed result (base64 with header): {result}")
        print(f"Result length: {len(result)} characters")
        
        # Decode to show structure
        decoded = base64.b64decode(result)
        header_byte = decoded[0]
        compressed_bytes = decoded[1:]
        
        print(f"\nHeader (bit length): {header_byte} bits")
        print(f"Compressed data size: {len(compressed_bytes)} bytes ({len(compressed_bytes) * 8} bits)")
        print(f"Total bitstring size: {len(decoded)} bytes")
        print(f"Compression ratio: {len(test_text.encode('utf-8')) / len(compressed_bytes):.2f}x")
